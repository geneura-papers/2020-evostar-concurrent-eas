%\documentclass[sigconf, authordraft]{acmart} -*- mode: Latex -*-
\documentclass[sigconf]{acmart}
\usepackage[utf8]{inputenc}
\usepackage{booktabs} % For formal tables
\usepackage{tikz}
\usepackage{pgfbaselayers}
\usepackage[underline=false]{pgf-umlsd} % for sequencediagram



% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.1145/nnnnnnn.nnnnnnn}

% ISBN
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}

% Conference
\acmConference[GECCO '20]{the Genetic and Evolutionary Computation Conference 2020}{July 8--12, 2020}{Cancun, Mexico}
\acmYear{2020}
\copyrightyear{2020}

%\acmArticle{4}
\acmPrice{15.00}
% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

\begin{document}

<<setup, cache=FALSE,echo=FALSE>>=
suppressPackageStartupMessages({
    library(ggplot2)
    library(ggthemes)
})
@

\title{Implementation matters, also in concurrent evolutionary algorithms}

%%% The submitted version for review should be ANONYMOUS
\author{Ben Trovato}
\authornote{Dr.~Trovato insisted his name be first.}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \postcode{43017-6221}
}
\email{trovato@corporation.com}

\author{G.K.M. Tobin}
\authornote{The secretary disavows any knowledge of this author's actions.}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \postcode{43017-6221}
}
\email{webmaster@marysville-ohio.com}

\author{Lars Th{\o}rv{\"a}ld}
\authornote{This author is the
  one who did all the really hard work.}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{B. Trovato et al.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  Concurrent evolutionary algorithms give researchers a chance to
  leverage the performance of powerful multi-core desktop
  architectures by parallelizing tasks using a high-level
  interface. However, this introduces additional complexity at the
  model and the implementation level. In this paper, we describe how
  to use parallel execution monitoring tools to check the effective
  parallelism of the implementation, and show how implementation-level
  improvements can translate in improvements at the algorithmic level,
  from the very basic evaluations/seconds point of view, to the
  possible scaling that can be achieved, up to a superlinear scaling
  in an off-the-shelf platform. We show how the design using
  communicating sequential processes implemented in the Raku language
  is the basis for these improvements.
\end{abstract}

\begin{CCSXML}
  <ccs2012>
<concept>
<concept_id>10003752.10003809.10003716.10011136.10011797.10011799</concept_id>
<concept_desc>Theory of computation~Evolutionary algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>

<concept>
<concept_id>10010520.10010521.10010537.10003100</concept_id>
<concept_desc>Computer systems organization~Cloud computing</concept_desc>
<concept_significance>500</concept_significance>
</concept>

<concept>
<concept_id>10010147.10010919.10010172</concept_id>
<concept_desc>Computing methodologies~Distributed algorithms</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}


\ccsdesc[500]{Theory of computation~Evolutionary algorithms}

\ccsdesc[300]{Computing methodologies~Distributed algorithms}

\keywords{Concurrent algorithms, distributed computing,
  event-based systems, stateless algorithms, CSP, Communicating Sequential Processes, 
  algorithm implementation, performance evaluation, distributed
  computing,  heterogeneous distributed systems,
  serverless computing, functions as a service.}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Concurrent computing gives us the capability to think about work to be done as a series of tasks whose life cycle overlap and are logically independent of each other. Concurrent languages and systems, besides, enables us to actually run those tasks in parallel, given the right combination of operating system and hardware. While not so long ago concurrent computing was the province of clusters or GPUs, current computing and operating systems, paired with concurrent languages such as Raku, Erlang, Scala, Rust, or Go, provide any practitioner with entry-level concurrent computing capabilities. As Rob Pike states in \cite{pike2012concurrency},
\begin{quote}
  Concurrency is about structure, parallelism is about execution.
\end{quote}

While concurrency deals with tasks as high-level structures and provides for ways in which these tasks can synchronize, it may or may not be executed at the same time using the facilities that the OS/system architecture provides.

However, despite concurrency-capable architectures being the prevalent kind of computer systems nowadays, concurrent evolutionary algorithms have not enjoyed the same kind of popularity. A bibliography search of concurrent evolutionary (or genetic) algorithms returns just a few hits, and even if we use another, lower-level term for it, multi-threaded evolutionary algorithms, we find just a few more, and in this case, focused on the use of the multiple threads in a GPU \cite{kromer2011comparison}.

% There might be several reasons for that. The main one is that conceiving a concurrent algorithm is not straightforward. A strategy for dividing work among different threads (every one in a task) and communication between them must be established; for starters tasks can be homogeneous or heterogeneous, or some space in between (with some tasks identical and some others doing other, different kind of work, for instance); they can also be synchronous (all tasks doing roughly the same amount of work, or exactly the same, with checkpoints where the main process waits for all tasks to be completed), or asynchronous with tasks starting, ending and communicating out of any pre-established sync mechanism.

% The above paragraph was difficult to read (a long sentence), here's an alternative:  
There might be several reasons for that. The main one is that conceiving a concurrent algorithm is not straightforward. We must define a strategy for dividing work among different threads (each one in a task), and the method of communication between them. A designer must consider the following. For starters, tasks can be homogeneous or heterogeneous, or something in between (with some tasks identical and others doing different kinds of work, for instance). Tasks can also be synchronous (all tasks doing roughly the same amount of work, or exactly the same, with checkpoints where the main process waits for all tasks to be completed), or asynchronous with tasks starting, ending and communicating out of any pre-established sync mechanism. - Mario


A secondary reason, in the case of evolutionary algorithms, is that, being as they are population-based algorithms that must keep a strict exploration-exploitation balance because anything affecting the population will tip the balance in one direction or another. Dividing the population will make the algorithm more exploitative, increasing the population more explorative; the more straightforward strategy of dividing population among threads will likely tip the balance in one direction or the other.
% The above sentence is a bit hard to read, maybe adding more detail, like dividing the population in smaller subsets, or increasing
% the population size - Mario
% Mario, do that directly, I guess it's better... - JJ


A third and probably more important reason, however, is the lack of tools to prove that the algorithm and its implementation are providing the kind of parallelism we really want, by keeping all cores busy running code, with no significant gaps. This makes measurements on the quality of the algorithm or decisions such as the right number of cores purely heuristic, with nothing more than the result to support it. 
% Is not clear what the antecedent is in "This makes measurments..."
% I meant scaling and things like that - JJ

So far, this was the path we had been following \cite{merelo:WEA:anon,Merelo:2018:MEA:3205651.3208317:anon}, with diverse results. Those papers proposed a concurrent evolutionary algorithm in Raku that distributed short-burst evolutionary processes in threads, with an additional mixer thread used for combining populations emanating from the different threads, and a communication system that sent a statistical representation of the population instead of the whole population. In particular, when scaling was apparently not happening, it was difficult to know if it was due to gaps in the use of processors, or to inherent problems with the algorithm itself.

In this paper, and after the release of a process logging and
monitoring tool for Raku concurrent programs within the Comma
integrated development environment, we analyze the correct working of
the parallel threads in the concurrent evolutionary algorithm; this
had not been possible so 
far, due to the lack of appropriate tools. This will lead to a
refactoring of the implementation, if needed, and a re-examination of
the scaling capabilities of the system using some benchmark function.

The rest of the paper is organized as follows: next, we examine the
state of the art in the area of parallel and concurrent evolutionary
algorithms, followed by a description of the methodology we have
followed in the experiments and the experimental setup we have created
for this in \ref{sec:design}. We will then present the results in the
next section (\ref{sec:res}), to be followed by the conclusions.

\section{State of the art}
\label{sec:soa}

Concurrency gives practitioners a practical way to tackle parallelism, and parallelism has been used in the evolutionary algorithms since early on. One of the first papers was published by Cohoon et al. \cite{cohoon1987punctuated}, and it already acknowledged the problems of the exploitation/exploration balance by proposing an algorithm based on {\em punctuated equilibrium}, that is, a balance that is regularly kept, but tipped from time to time. In the distributed architecture they propose, they consider that a parallel evolutionary algorithm should not only aim at being faster by doing the same amount of things in parallel but try to ``find better solutions with less total work'', that is, the distributed architecture itself will have an algorithmic role and influence positively the performance of the evolutionary algorithm. Ultimately, it's a synchronous island model with random migration between islands, but the underlying ideas seem surprisingly modern and relevant. 


The lack of suitably systems impeded advancement on this subject, but the 90s saw the introduction of the Inmos Transputer, a processor architecture designed for parallelism, paired with a programming language called Occam that took advantage of that concurrency.  East and McFarlane used it for evolutionary algorithms, \cite{east1993implementation}. They emphasize the importance of implementation, by stating that (inherent) parallelism in the algorithm must be considered separately from its implementation. In the case of population-based algorithms like EA, the obvious approach seems to be to divide the population, or somehow evaluate it in parallel. That is not necessarily the most efficient way, and they mention the possibility of modifying the algorithm to ``render (the) implementation more efficient''. Interestingly, they introduce the so-called communicating process architecture, an implementation of Hoare's Communicating Sequential Processes model \cite{Hoare:1978:CSP:359576.359585} where tasks communicate only through {\em channels}. They do examine the efficiency of every one of the processing elements, realizing that when 8 processors are used, it goes down a bit to 95\%. But their striking result is that the right parallel implementation provides a boost to the performance of the algorithm itself. Implementation matters \cite{DBLP:conf/iwann/MereloRACML11}, and by leveraging its capabilities and using the right paradigm (as opposed to a direct reinterpretation of the original sequential algorithm, running in parallel), algorithmic gains can be readily obtained.

It was not until the introduction of concurrent languages on commodity hardware that this paradigm really started to catch on, with EAs using channels implemented in Erlang \cite{AmandaBienz2011,kerdprasop2012concurrent,barwell2017using,DBLP:conf/gecco/CruzGGC13}, Go \cite{PEDROSO2017995} or Raku (Perl 6) \cite{DBLP:conf/gecco/GuervosV18}. The different implementations reflect the facilities provided by the different languages, but also show how they try to measure the effectivity of the concurrency and parallelism on the algorithm. Bienz et al. \cite{AmandaBienz2011} first propose different algorithms, one of them works on a ``grid'', with processes only communicating with others in neighboring nodes of the grid; but they also test on different types of hardware, with 4 and 16 core systems. They show, for instance, that including a {\em master} task that distributes work blurs the distinction between the number of processors, hindering scaling; but due to a lack of tools, they can't really find out if it is due to tasks sitting idle waiting for others; in the same way, Kerdprasop et al. \cite{kerdprasop2012concurrent} test the limits of concurrency by showing that performance might suffer when the number of tasks goes over a certain limit (which might be related to the hardware used). Incidentally, the scaling shown by their application is rather poor, with the numbers of threads not decreasing significantly the wallclock time taken by the algorithm; this again shows the need to delve into the actual use by the implementation of the parallel facilities through the concurrent structures provided by the language. The solution proposed by Pedroso et al., \cite{PEDROSO2017995}, uses parallel tasks that communicate via channels, which is a default feature in Go, but there are checkpoints where all tasks are awaited and the code runs sequentially. Even so, they report a good, even superlinear, speedup for up to roughly 15 processors. This is a characteristic of many parallel implementations, something that was observed quite early on \cite{Alba02parallelevolutionary} and which is due to carefully keeping diversity high. In this case, however, there's no examination of the implementation beyond heuristic measurements.

In this paper, we will try to use that kind of tools to find out the source of the bottleneck in previous implementations of the concurrent evolutionary algorithm and, from that foundation, improve it to achieve superlinear scaling given the right hardware platform.


\section{Design of a concurrent evolutionary algorithm in Raku}
\label{sec:design}

Raku is a concurrent, functional language
\cite{merelo2019running} (formerly known as Perl 6) which was conceived with the
intention of providing a solid conceptual framework for multi-paradigm
computing, including thread-based concurrency and asynchrony. It's got
a heuristic layer that optimizes code during execution time. In the last few
years, the performance of programs written in Raku has been sped-up by a 100x factor, 
approaching the same scale of other interpreted languages, although there is still  some room for improvement.

The {\tt Algorithm::Evolutionary::Simple} Raku module was published
in the ecosystem in 2018; in March 2019 version 0.0.8 was released. It
is a straightforward implementation of a canonical evolutionary
algorithm with binary representation and includes building blocks for
a generational genetic algorithm, as well as some fitness functions
used generally as benchmarks. It uses best practices to obtain the maximum performance out of the genetic operators that are implemented.

The baseline we are building upon, is similar to the one used in previous experiments
\cite{Merelo:2018:MEA:3205651.3208317:anon}. We intended to
create a system that was a clean slate implementation of concurrent
evolutionary algorithms, following the principles of CSP
(communicating sequential processes) \cite{Hoare:1978:CSP:359576.359585}. We decided to allow the algorithm to implement several threads communicating state through
channels. Every process itself will be stateless, reacting to messages arriving 
in the channels it is listening to and sending 
results back to them without changing state; that way, they appear as
sequential tasks, although as a matter of fact low-level threads are
created and then ditched when the task is done.

As in the previous papers \cite{merelo:WEA:anon}, 
we will use two kinds of threads and two channels:  
\begin{itemize}
\item The {\em evolutionary} threads will be the ones performing 
the operations of the evolutionary algorithm; they will run an
evolutionary algorithm (based on the aforementioned library, {\tt
  Algorithm::Evolutionary::Simple}) for 16 generations or until the
solution is found.
\item The {\em mixing} thread will take existing populations, to create
  new ones as a mixture of them.
\end{itemize}

\begin{figure}[h!tb]
  %\centering
  \vspace{-.5\intextsep}
\hspace*{-.8\columnsep}
\includegraphics[width=0.95\columnwidth]{imgs/popmixer}
\caption{General scheme of operation of channels and thread groups. }
\label{fig:scheme}
\end{figure}


Besides, the two channels carry messages consisting of populations,
but they do so in a different way:\begin{itemize}
  
\item The {\em evolutionary} channel will be used for carrying
  non-evolved, or newly generated, populations.
\item The {\em mixer} channel will carry {\em pairs} of populations.  
\end{itemize}

These will be connected as shown in Figure \ref{fig:scheme}. The
evolutionary thread group will read only from the evolutionary channel,
evolve for a number of generations, and send the result to the mixer
channel; the mixer group of threads will read only from the mixer
channel, in pairs. From every pair, a random element is put back into
the mixer channel, and a new population is generated and sent back to
the evolutionary channel. 

\begin{figure}[h!tb]
  \centering
  \vspace{-.5\intextsep}
\hspace*{-.8\columnsep}
\scalebox{.8}{
\begin{sequencediagram}

\newthread[red]{E}{Evolver} 

\tikzstyle{inststyle}+=[rounded corners=3mm] 
\newinst{C}{Channel}

\tikzstyle{inststyle}+=[rounded corners=0]
\newthread[blue]{M}{Mixer}

\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_1$}{C} 
\mess{C}{$pop_1$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_2$}{C}  
\mess{C}{$pop_2$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{}\end{call}

\postlevel\postlevel
\setthreadbias{west}
\begin{messcall}{M}{\shortstack{ \{\ $mixpop_1$,\\ $mixpop_2$,\\ \vdots \\ $mixpop_k$ \} }}{C}

\mess{C}{$mixpop_1$}{E} 
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_3$}{C} 
\postlevel
\mess{C}{$mixpop_2$}{E} 
%\prelevel
\mess{C}{$pop_3$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_4$}{C}
\mess{C}{$pop_4$}{M}
\end{messcall}

\setthreadbias{west}
\prelevel
\mess{C}{$mixpop_k$}{E}%\end{messcall}

\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{\vdots}\end{call}
\prelevel
\begin{call}{E}{evolve()}{E}{\vdots}\end{call}

\end{sequencediagram}
}

\caption{Communication between threads and channels for
  concurrent EAs. The two central bars represent the channel, and
  color corresponds to their roles: blue for mixer, red for evolver. 
  Notice how the evolver threads always read from the mixer channel, 
  and always write to the evolver channel.}
\label{fig:schematic}
% \end{figure}
\end{figure}

The main objective of using two channels is
to avoid deadlocks; the fact that one population is always written
back to the mixer channel makes it avoid starvation.
Figure \ref{fig:schematic} illustrates this operation, where the
timeline of the interchange of messages between the evolver and mixer
threads and evolver and mixer channels is clarified.

The state of the algorithm will be transmitted via messages that
contain data about one population. Since using the whole population
will incur in a lot of overhead, we use a strategy that is inspired in {\em EDA}, 
or Estimation of Distribution Algorithm: instead of 
transmitting the entire population, the message sent to the channel 
will consist of an array that contains the probability distribution 
across each gene in the population, a strategy used in Estimation of
Distribution Algorithms \cite{larranaga2001estimation}; in this
context, this strategy is similar to the one presented by de la Ossa et
al. in \cite{10.1007/978-3-540-30217-9_25}; however, our strategy differs from a pure EDA in that once the evolutionary
thread has internally run a canonical genetic algorithm, it takes 
only the top quartile of best individuals to compute an array with the 
probability distribution of their genes (computed with frequentist rules) 
and then compose the message that is sent to the {\em mixer} threads. 

A {\em mixer} thread, in turn, 
builds a new prototype array by choosing randomly at each gene location
one probability parameter out of the  two distributions, 
instead of working directly on individuals. When one evolver thread
reads the message with the population distribution, it generates a new population 
using a distribution from the mixer channel; this can be either one
mixed population, or simply a population that was sent back, as is, to
the mixer channel and is evolved further. This strategy of sending a
population distribution was used to avoid the overhead of sending
several elements of the population; although the overhead increases
with the population size, it is not much heavier than sending a single
individual, and carries much more information. In previous papers,
this was found to be the strategy with better scaling and overall results.

However, the main purpose of this paper was to assess hot parallel
execution was actually taking place.
The introduction of the parallel execution monitor in Comma by
Edument, the company that sells it, along with a module, {\tt Log::Timeline}, that needs to be added to the program in order for it to be able to visualize the results, gives us the capability of seeing events as they are happening in a unified timeline, and also check the duration and simultaneity of the tasks occurring in different threads. Please refer to Figure \ref{fig:comma}, which is a capture of this tool running on the program presented in this paper.

\begin{figure*}[h!tb]
  \centering
  \includegraphics[width=1.9\columnwidth]{imgs/comma}
  \caption{Comma monitor for parallel execution. The $x$ axis is the timeline in seconds, while several concurrent tasks and events are represented in the $y$ axis. The two stripes in different shades of green are the two threads that are running an evolutionary algorithm; the change of color indicates that the task has finished and started a new with new data from the communication channel that conveys populations.}
    \label{fig:comma}
    \end{figure*}

    This initial visualization already highlighted some problems: there were gaps where no thread was being executed, they run almost in parallel, and there were also some {\em short} runs for which we had no explanation. We'll interpret this and show how it was fixed right next.

    \section{Implementation matters: improving the implementation of a concurrent evolutionary algorithm}

By using the logging tool to register (and visualize) events, we realized that the {\tt while} loop we were using for the evolutionary algorithm within every task sometimes finished before the condition was met, that is, before either the number of generations (16) had been reached, or the solution had been found.

This revealed one of the problems of concurrent implementations: using some shared information, in this case, the number of generations that the evolutionary algorithm has run, can be tricky, and the optimizations that the language runs internally can sometimes throw loops off whack, as was the case. Besides, the register revealed an additional problem: not only the loops running the EA sometimes finished before they should, they generally finished much later than they should have. That meant that the EA was running many more generations, getting stuck in local optima, which was making the implementation {\em worse} than the algorithm should have been. This, again, proves that implementation matters and that best practices must be followed to find the best implementation match for the algorithm features.

In this case, the improvement consisted in changing the {\tt while} loop to a {\tt for} loop. The loop variable was running over the required number of generations, and exceptionally it was getting out of the loop when the solution was found. After doing this, we used again the Comma monitor on the program, with a result similar to the one shown in Figure \ref{fig:comma2}. 
%
\begin{figure*}[h!tb]
  \centering
  \includegraphics[width=1.9\columnwidth]{imgs/comma}
  \caption{Comma monitor for parallel execution after changing to a {\tt for} loop. It shows in the vertical axis the two threads running, as well as the third one, the mixer thread. The arrows represent events, with {\em Weird} indicating when the loop has exited before it should.}
    \label{fig:comma2}
  \end{figure*}
  % 
  % One of the things that can be observed is that the whole run takes less time.   Before it never finished before the 16 seconds that were shown in the default window, now it always takes shorter for the EA to find the solution to the Leading Ones problem.
  % % Proposed change:
  One of the things that can be observed is that the whole run takes less time in 
  this new version. Before, it was never able to finish below the 16 seconds that 
  were shown in the default window, and now it always takes less time 
  for the EA to find the solution to the Leading Ones problem

  Before, the two threads were initiated at the same time and ended the 16 generation at the same time, which did not really make sense, since there's a small overhead involved in starting them. Now it's clear that the second thread starts a bit after the first, and that they drift away after that. The solution is found by one of them after 9 seconds, and communication to channels, shown by arrows, takes place asynchronously, as well as the (very short) execution of the mixer (barely seen in the lowest stripe). There are no gaps in the evolutionary stripes, so those threads are used to its full extent, all the time.

  This improvement in the execution drove us to try, now, measuring the scaling capabilities once we increase the number of threads. We'll see the results next.

  \section{Results}
  \label{sec:res}
  
  We will first apply the new version of the concurrent evolutionary algorithm to the leading ones problem, which previously had not reached a good solution. We will do so in laptop with 4 cores, a Lenovo X1, using Raku v2020.02 on Ubuntu 16.04. Code is available in GitHub under a free license. We tested two different (total) populations, 512 and 256; as indicated in the methodology, population will be divided evenly among the existing processors.
  
\begin{figure}[h!tb]
<<laptop, cache=FALSE,echo=FALSE>>=
data <- read.csv("code/lo-evosoft-results.csv")
data$Population <- as.factor(data$Population)
data$Threads <- as.factor(data$Threads)
data$Evaluation.rate <- data$Evaluations/data$Time
ggplot(data,aes(x=Threads,y=Evaluations,color=Population))+geom_tufteboxplot()+theme_tufte()
@
  \caption{Boxplot of the number of evaluations for the two population sizes and threads up to 6. }
  \label{fig:eval1}
 \end{figure} 
 %

 The first chart (in Figure \ref{fig:eval1} shows one of the quantities we are interested in, number of evaluations; distributed algorithms, as indicated in the early works about them, should help improve the quality, not only the time. Population has definitely an influence on it, but the most interesting thing is that the number of evaluations actually {\em decreases} when the number of threads increases; in this case, we went up to 6, or a total or 7 for the program, since the physical hardware included only 8 cores. There's also a difference with population size: bigger population is more explorative, needs more evaluations, but additionally, it gets worse with the number of threads, implying that there's a right size for the total population, something that has been already observed in other works of this kind.
 
\begin{figure}[h!tb]
<<laptop2, cache=FALSE,echo=FALSE>>= 
ggplot(data,aes(x=Threads,y=Time,color=Population))+geom_tufteboxplot()+theme_tufte()+scale_y_log10()
@
  \caption{Boxplot of the time for the two population sizes and threads up to 6. }
  \label{fig:eval2}
\end{figure}
%

Less evaluations should already mean less time needed to reach the solution, but we should get additional boost by the parallel execution, and this is plotted in Figure \ref{fig:eval2}. In this case there's a good scaling down of time not only from 2 to 4, but also from 4 to 6, when the number of evaluations is roughly the same, indicating that those evaluations are taking place in parallel, as indicated by the monitor. The curve is more steep than for the number of evaluations.
%
\begin{figure}[h!tb]
<<laptop3, cache=FALSE,echo=FALSE>>= 
ggplot(data,aes(x=Threads,y=Evaluation.rate,color=Population))+geom_tufteboxplot()+theme_tufte()
@
  \caption{Boxplot of evaluation rate for the two population sizes and threads up to 6.}
  \label{fig:eval3}
\end{figure}
%
The last figure of this series, Figure \ref{fig:eval3}, shows how the number of evaluations per second keeps increasing to around 4k per second, although once again the curve is not so steep from 4 to 6. It's {\em better} for population = 256, which probably indicates some overhead in communication due to the population size.

This first result was promising, so we tried again using more powerful hardware, a system with 16 total cores and Ubuntu 18.04, using the same Raku version, 2020.02. The number of evaluations is shown in Figure \ref{fig:freija:1}. In this case we used up to 8 threads (+ 1 for the mixer). The length of the leading ones problem was increased up to 60, and we also increased the population up to 1024, once again divided evenly between all threads. This boxplot shows that the number of evaluations needed decreases with the number of threads, once again showing the algorithmic value of dividing the population into several asynchronous threads; although, again, the gain from 6 to 8 threads is not too much. This is probably due to the fact that in this case population per thread is 128, which is a small population and might tend to get stuck more easily than for bigger populations.
%
\begin{figure}[h!tb]
<<laptop4, cache=FALSE,echo=FALSE>>=
data <- read.csv("code/lo-evosoft-results-l60.csv")
data$Population <- as.factor(data$Population)
data$Threads <- as.factor(data$Threads)
data$Evaluation.rate <- data$Evaluations/data$Time
ggplot(data,aes(x=Threads,y=Evaluations))+geom_tufteboxplot()+theme_tufte()
@
  \caption{Second experiment, leading ones with length 60, population 1024, boxplot of the number of evaluations for the two population sizes and threads up to 8. }
  \label{fig:freija:1}
 \end{figure} 
 %
However, this is overcome by the fact that evaluations are taking place in parallel, and thus the time needed to find the solution decreases steadily from 2 to 8 threads, as shown in Figure \ref{fig:freija:2}. Please note that in this case, the axis is logarithmic, which shows a superlinear decrease in time for 2 to 4, with median time taking almost an order of magnitude less in that case, and steep decreases in the others.
%
\begin{figure}[h!tb]
<<laptop5, cache=FALSE,echo=FALSE>>= 
ggplot(data,aes(x=Threads,y=Time))+geom_tufteboxplot()+theme_tufte()+scale_y_log10()
@
  \caption{Second experiment, boxplot of the time for the two population sizes and threads up to 8. The $y$ axis is logarithmic.}
  \label{fig:freija:2}
\end{figure}
%
And this is due to what we show in Figure \ref{fig:freija:3}: a steady, linear, increase in the number of evaluations per second due to the concurrent implementation. The double treat of better algorithmic performance, with less evaluations needed to find the solution, and the fact that they are done in parallel, takes us up to almost 10k evaluations per second in the best case.
%
\begin{figure}[h!tb]
<<laptop6, cache=FALSE,echo=FALSE>>= 
ggplot(data,aes(x=Threads,y=Evaluation.rate))+geom_tufteboxplot()+theme_tufte()
@
  \caption{Second experiment, boxplot of evaluation rate for the two population sizes and threads up to 8.}
  \label{fig:freija:3}
\end{figure}
%

Please note that raw performance is not (all) we were looking for, but for the combination of the two brought by an efficient concurrent implementation as well as an algorithmic model that favors diversity by not sending always the best individual (but a mathematical representation of the best, equivalent to an EDA), asynchronous execution and letting every thread run independently for a fixed amount of generations.

\section{Conclusions and discussion}
\label{sec:conclusions}

In this paper we set out to prove how using the right tools to implement and analyze a concurrent evolutionary algorithm could help us to obtain an algorithm that is able to find the solution using less overall computational effort (less evaluations) as well as less time thanks to an efficient parallel implementation of the concurrent processes.

We used Raku, a new-generation language that uses a custom virtual machine to implement concurrency, as well as other paradigms such as functional programming, also used heavily in concurrency; on top of that, we used the integrated development environment Comma, which provides a concurrency monitor, that we have used to hone in on the implementation to get the parallelism working correctly. This new implementation needs to be tested extensively, but seems to be not only faster, but also avoids errors that made the previous one slower (and less efficient) than it should be.

The new experiments show that getting the right population not only at the top level, but also all the way down to the biggest number of threads, is essential for a good scaling in the number of evaluations that, in and by itself, would already provide a performance boost even if the concurrency is not equivalent to the actual number of physical parallel threads that are available. However, combined with the current implementation that actually leverages physical available threads, it can really bring, at least for part of the scale, superlinear speedups for concurrent evolutionary algorithms, at least for problems of this kind.

This result could be generalized to other languages of the same kind, such as Go, Julia or Erlang, any language, in fact, which is able to use channels for communication between high-level threads represented as promises. Using languages that are faster (like Go, for instance) could probably bring much more performance if evaluation actually takes a lot of time. Since we are using a binary EDA-like {\em migration} operator, this can be extended also to other problems that can be represented using a binary string.

This new research avenue opens many different lines of work, as many as the degrees of freedom there are in this new approach. One that we might find promising is to use dissimilar threads so that we test the {\em intermediate disturbance hypothesis}: more diversity arises if an evolutionary system is subjected to a disturbance that is big enough to be noticed, but not big enough to destroy it; dissimilar threads should follow this pattern.

But there are many other possible extensions to this work: the migration algorithm could be extended to other data structures, or could be changed in many different ways, as long as it provides low overhead for communication between different threads. Using more threads would be possible in late-generation processors, and finding the right population for working with them would be a challenge, as well as simply avoiding that step and letting the system, somehow, self-organize and find the right population for every specific problem. All these research avenues are left as future work.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgements}

We are grateful to Jonathan Worthington and the rest of the Edument team for their disposition to help with implementation problems and provide suggestions to make this work correctly. This is extended to the rest of the Raku development team, which is an excellent and technically knowledgeable community committed to creating a great language.

Hidden for double-blind review
% This paper has been supported in part by projects DeepBio (TIN2017-85727-C4-2-P).

\bibliographystyle{ACM-Reference-Format}
\bibliography{geneura,concurrent}

\end{document}
