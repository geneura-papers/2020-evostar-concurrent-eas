%\documentclass[sigconf, authordraft]{acmart} -*- mode: Latex -*-
\documentclass[sigconf]{acmart}
\usepackage[utf8]{inputenc}
\usepackage{booktabs} % For formal tables
\usepackage{tikz}
\usepackage{pgfbaselayers}
\usepackage[underline=false]{pgf-umlsd} % for sequencediagram



% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.1145/nnnnnnn.nnnnnnn}

% ISBN
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}

% Conference
\acmConference[GECCO '20]{the Genetic and Evolutionary Computation Conference 2020}{July 8--12, 2020}{Cancun, Mexico}
\acmYear{2020}
\copyrightyear{2020}

%\acmArticle{4}
\acmPrice{15.00}
% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

\begin{document}

<<setup, cache=FALSE,echo=FALSE>>=
suppressPackageStartupMessages({
    library(ggplot2)
    library(ggthemes)
})
@

\title{Implementation matters, also in concurrent evolutionary algorithms}

%%% The submitted version for review should be ANONYMOUS
\author{Ben Trovato}
\authornote{Dr.~Trovato insisted his name be first.}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \postcode{43017-6221}
}
\email{trovato@corporation.com}

\author{G.K.M. Tobin}
\authornote{The secretary disavows any knowledge of this author's actions.}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \postcode{43017-6221}
}
\email{webmaster@marysville-ohio.com}

\author{Lars Th{\o}rv{\"a}ld}
\authornote{This author is the
  one who did all the really hard work.}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{B. Trovato et al.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  Concurrent evolutionary algorithms give researchers a chance to leverage the performance of powerful multi-core desktop architectures by parallelizing tasks using a high-level interface. However, this introduces additional complexity at the model and the implementation level. In this paper, we describe how to use parallel execution monitoring tools to check the effective parallelism of the implementation, and show how implementation-level improvements can translate in improvements at the algorithmic level, from the very basic evaluations/seconds point of view, to the possible scaling that can be achieved, up to a superlinear scaling in an off-the-shelf platform. We show how the design using communicating sequential processes implemented in the language Raku is the basis for these improvements.
\end{abstract}
% Changed implementation to design, check if this is true - Mario
\begin{CCSXML}
  <ccs2012>
<concept>
<concept_id>10003752.10003809.10003716.10011136.10011797.10011799</concept_id>
<concept_desc>Theory of computation~Evolutionary algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>

<concept>
<concept_id>10010520.10010521.10010537.10003100</concept_id>
<concept_desc>Computer systems organization~Cloud computing</concept_desc>
<concept_significance>500</concept_significance>
</concept>

<concept>
<concept_id>10010147.10010919.10010172</concept_id>
<concept_desc>Computing methodologies~Distributed algorithms</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}


\ccsdesc[500]{Theory of computation~Evolutionary algorithms}

\ccsdesc[300]{Computing methodologies~Distributed algorithms}

\keywords{Concurrent algorithms, distributed computing,
  event-based systems, stateless algorithms, CSP, Communicating Sequential Processes, 
  algorithm implementation, performance evaluation, distributed
  computing,  heterogeneous distributed systems,
  serverless computing, functions as a service.}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Concurrent computing gives us the capability to think about work to be done as a series of tasks whose life cycle overlap and are logically independent of each other. Concurrent languages and systems, besides, enables us to actually run those tasks in parallel, given the right combination of operating system and hardware. While not so long ago concurrent computing was the province of clusters or GPUs, current computing and operating systems, paired with concurrent languages such as Raku, Erlang, Scala, Rust, or Go, provide any practitioner with entry-level concurrent computing capabilities. As Rob Pike states in \cite{pike2012concurrency},
\begin{quote}
  Concurrency is about structure, parallelism is about execution.
\end{quote}

While concurrency deals with tasks as high-level structures and provides for ways in which these tasks can synchronize, it may or may not be executed at the same time using the facilities that the OS/system architecture provides.

However, despite concurrency-capable architectures being the prevalent kind of computer systems nowadays, concurrent evolutionary algorithms have not enjoyed the same kind of popularity. A bibliography search of concurrent evolutionary (or genetic) algorithms returns just a few hits, and even if we use another, lower-level term for it, multi-threaded evolutionary algorithms, we find just a few more, and in this case, focused on the use of the multiple threads in a GPU \cite{kromer2011comparison}.

There might be several reasons for that. The main one is that conceiving a concurrent algorithm is not straightforward. A strategy for dividing work among different threads (every one in a task) and communication between them must be established; for starters tasks can be homogeneous or heterogeneous, or some space in between (with some tasks identical and some others doing other, different kind of work, for instance); they can also be synchronous (all tasks doing roughly the same amount of work, or exactly the same, with checkpoints where the main process waits for all tasks to be completed), or asynchronous with tasks starting, ending and communicating out of any pre-established sync mechanism.

% The above paragraph was difficult to read (a long sentence), here's an alternative:  
%There might be several reasons for that. The main one is that conceiving a concurrent algorithm is not straightforward. We must define a strategy for dividing work among different threads (each one in a task), and the method of communication between them. A designer must consider the following. For starters, tasks can be homogeneous or heterogeneous, or something in between (with some tasks identical and others doing different kinds of work, for instance). Tasks can also be synchronous (all tasks doing roughly the same amount of work, or exactly the same, with checkpoints where the main process waits for all tasks to be completed), or asynchronous with tasks starting, ending and communicating out of any pre-established sync mechanism. - Mario


A secondary reason, in the case of evolutionary algorithms, is that, being as they are population-based algorithms that must keep a strict exploration-exploitation balance becuase anything affecting the population will tip the balance in one direction or another. Dividing the population will make the algorithm more exploitative, increasing the population more explorative; the more straightforward strategy of dividing population among threads will likely tip the balance in one direction or the other.
% The above sentence is a bit hard to read, maybe adding more detail, like dividing the population in smaller subsets, or increasing
% the population size - Mario 


A third and probably more important reason, however, is the lack of tools to prove that the algorithm and its implementation are providing the kind of parallelism we really want, by keeping all cores busy running code, with no significant gaps. This makes measurements or decisions such as the right number of cores purely heuristic, with nothing more than the result to support it. 
% Is not clear what the antecedent is in "This makes measurments..." 

So far, this was the path we had been following \cite{merelo:WEA:anon,Merelo:2018:MEA:3205651.3208317:anon}, with diverse results. Those papers proposed a concurrent evolutionary algorithm in Raku that distributed short-burst evolutionary processes in threads, with an additional mixer thread used for combining populations emanating from the different threads, and a communication system that sent a statistical representation of the population instead of the whole population. In particular, when scaling was apparently not happening, it was difficult to know if it was due to gaps in the use of processors, or to inherent problems with the algorithm itself.

In this paper, after the release of a process logging and monitoring tool for Raku concurrent programs within the Comma integrated development environment, we examine the correct working of the concurrent evolutionary algorithm; this had not been possible so far, due to the lack of appropriate tools. This will lead to a refactoring of the implementation, if needed, and a re-examination of the scaling capabilities of the system.

The rest of the paper is organized as follows: next, we examine the state of the art in the area of parallel and concurrent evolutionary algorithms, followed by a description of the methodology we have followed in the experiments and the experimental setup we have created for this in \ref{sec:design}. We will then present the results in the next section, to be followed by the conclusions.

\section{State of the art}
\label{sec:soa}

Concurrency gives practitioners a practical way to tackle parallelism, and parallelism has been used in the evolutionary algorithms since early on. One of the first papers was published by Cohoon et al. \cite{cohoon1987punctuated}, and it already acknowledged the problems of the exploitation/exploration balance by proposing an algorithm based on {\em punctuated equilibrium}, that is, a balance that is regularly kept, but tipped from time to time. In the distributed architecture they propose, they consider that a parallel evolutionary algorithm should not only aim at being faster by doing the same amount of things in parallel but try to ``find better solutions with less total work'', that is, the distributed architecture itself will have an algorithmic role and influence positively the performance of the evolutionary algorithm. Ultimately, it's a synchronous island model with random migration between islands, but the underlying ideas seem surprisingly modern and relevant. 


The lack of suitably systems impeded advancement on this subject, but the 90s saw the introduction of the Inmos Transputer, a processor architecture designed for parallelism, paired with a programming language called Occam that took advantage of that concurrency.  East and McFarlane used it for evolutionary algorithms, \cite{east1993implementation}. They emphasize the importance of implementation, by stating that (inherent) parallelism in the algorithm must be considered separately from its implementation. In the case of population-based algorithms like EA, the obvious approach seems to be to divide the population, or somehow evaluate it in parallel. That is not necessarily the most efficient way, and they mention the possibility of modifying the algorithm to ``render (the) implementation more efficient''. Interestingly, they introduce the so-called communicating process architecture, an implementation of Hoare's Communicating Sequential Processes model \cite{Hoare:1978:CSP:359576.359585} where tasks communicate only through {\em channels}. They do examine the efficiency of every one of the processing elements, realizing that when 8 processors are used, it goes down a bit to 95\%. But their striking result is that the right parallel implementation provides a boost to the performance of the algorithm itself. Implementation matters \cite{DBLP:conf/iwann/MereloRACML11}, and by leveraging its capabilities and using the right paradigm (as opposed to a direct reinterpretation of the original sequential algorithm, running in parallel), algorithmic gains can be readily obtained.

It was not until the introduction of concurrent languages on commodity hardware that this paradigm really started to catch on, with EAs using channels implemented in Erlang \cite{AmandaBienz2011,kerdprasop2012concurrent,barwell2017using,DBLP:conf/gecco/CruzGGC13}, Go \cite{PEDROSO2017995} or Raku (Perl 6) \cite{DBLP:conf/gecco/GuervosV18}. The different implementations reflect the facilities provided by the different languages, but also show how they try to measure the effectivity of the concurrency and parallelism on the algorithm. Bienz et al. \cite{AmandaBienz2011} first propose different algorithms, one of them works on a ``grid'', with processes only communicating with others in neighboring nodes of the grid; but they also test on different types of hardware, with 4 and 16 core systems. They show, for instance, that including a {\em master} task that distributes work blurs the distinction between the number of processors, hindering scaling; but due to a lack of tools, they can't really find out if it is due to tasks sitting idle waiting for others; in the same way, Kerdprasop et al. \cite{kerdprasop2012concurrent} test the limits of concurrency by showing that performance might suffer when the number of tasks goes over a certain limit (which might be related to the hardware used). Incidentally, the scaling shown by their application is rather poor, with the numbers of threads not decreasing significantly the wallclock time taken by the algorithm; this again shows the need to delve into the actual use by the implementation of the parallel facilities through the concurrent structures provided by the language. The solution proposed by Pedroso et al., \cite{PEDROSO2017995}, uses parallel tasks that communicate via channels, which is a default feature in Go, but there are checkpoints where all tasks are awaited and the code runs sequentially. Even so, they report a good, even superlinear, speedup for up to roughly 15 processors. This is a characteristic of many parallel implementations, something that was observed quite early on \cite{Alba02parallelevolutionary} and which is due to carefully keeping diversity high. In this case, however, there's no examination of the implementation beyond heuristic measurements.

In this paper, we will try to use that kind of tools to find out the source of the bottleneck in previous implementations of the concurrent evolutionary algorithm and, from that foundation, improve it to achieve superlinear scaling given the right hardware platform.


\section{Design of a concurrent evolutionary algorithm in Raku}
\label{sec:design}

Raku is a concurrent, functional language
\cite{merelo2019running} (formerly known as Perl 6) which was conceived with the
intention of providing a solid conceptual framework for multi-paradigm
computing, including thread-based concurrency and asynchrony. It's got
a heuristic layer that optimizes code during execution time. In the last few
years, performance of programs written in Raku has been sped-up by a 100x factor, 
approaching the same scale of other interpreted
languages, although still with some room for improvement.

The {\tt Algorithm::Evolutionary::Simple} Raku module was published
in the ecosystem a year ago and got recently into version 0.0.7. It
is a straightforward implementation of a canonical evolutionary
algorithm with binary representation and includes building blocks for
a generational genetic algorithm, as well as some fitness functions
used generally as benchmarks.

% Those evolutionary algorithm building blocks do not include concurrent
% primitives; it's up to the developer to design a concurrent
% evolutionary algorithm using it.

The baseline we are building upon, is similar to the one used in previous experiments
\cite{Merelo:2018:MEA:3205651.3208317:anon}. Our intention was to
create a system that was not functionally equivalent to a sequential
evolutionary algorithms, that also follows the principle of
CSP. We decided to allow the algorithm to implement several threads communicating state through
channels. Every process itself will be stateless, reacting to the
% Please consider for other papers: Every 'subroutine' itself will be stateless, reacting to the... - Mario
presence of messages in the channels it is listening to and sending
result back to them, without changing state.

As in the previous papers \cite{merelo:WEA:anon}, 
we will use two groups of threads and two channels. 
The two groups of threads perform the following functions:\begin{itemize}
\item The {\em evolutionary} threads will be the ones performing 
the operations of the evolutionary algorithm.
\item The {\em mixing} thread will take existing populations, to create
  new ones as a mixture of them.
\end{itemize}

\begin{figure}[h!tb]
  %\centering
  \vspace{-.5\intextsep}
\hspace*{-.8\columnsep}
\includegraphics[width=0.95\columnwidth]{imgs/popmixer}
\caption{General scheme of operation of channels and thread groups. }
\label{fig:scheme}
\end{figure}


Besides, the two channels carry messages consisting of populations,
but they do so in a different way:\begin{itemize}
  
\item The {\em evolutionary} channel will be used for carrying
  non-evolved, or generated, populations.
\item The {\em mixer} channel will carry, {\em in pairs}, evolved
  populations. 
\end{itemize}

These will be connected as shown in Figure \ref{fig:scheme}. The
evolutionary thread group will read only from the evolutionary channel,
evolve for a number of generations, and send the result to the mixer
channel; the mixer group of threads will read only from the mixer
channel, in pairs. From every pair, a random element is put back into
the mixer channel, and a new population is generated and sent back to
the evolutionary channel. 

\begin{figure}[h!tb]
  \centering
  \vspace{-.5\intextsep}
\hspace*{-.8\columnsep}
\scalebox{.6}{
\begin{sequencediagram}

\newthread[red]{E}{Evolver} 

\tikzstyle{inststyle}+=[rounded corners=3mm] 
\newinst{C}{Channel}

\tikzstyle{inststyle}+=[rounded corners=0]
\newthread[blue]{M}{Mixer}

\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_1$}{C} 
\mess{C}{$pop_1$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_2$}{C}  
\mess{C}{$pop_2$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{}\end{call}

\postlevel\postlevel
\setthreadbias{west}
\begin{messcall}{M}{\shortstack{ \{\ $mixpop_1$,\\ $mixpop_2$,\\ \vdots \\ $mixpop_k$ \} }}{C}

\mess{C}{$mixpop_1$}{E} 
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_3$}{C} 
\postlevel
\mess{C}{$mixpop_2$}{E} 
%\prelevel
\mess{C}{$pop_3$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_4$}{C}
\mess{C}{$pop_4$}{M}
\end{messcall}

\setthreadbias{west}
\prelevel
\mess{C}{$mixpop_k$}{E}%\end{messcall}

\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{\vdots}\end{call}
\prelevel
\begin{call}{E}{evolve()}{E}{\vdots}\end{call}

\end{sequencediagram}
}

\caption{Communication between threads and channels for
  concurrent EAs. The two central bars represent the channel, and
  color corresponds to their roles: blue for mixer, red for evolver. 
  Notice how the evolver threads always read from the mixer channel, 
  and always write to the evolver channel.}
\label{fig:schematic}
% \end{figure}
\end{figure}

The main objective of using two channels is
to avoid deadlocks; the fact that one population is written always
back to the mixer channel avoids starvation of the channel. 
Figure \ref{fig:schematic} illustrates this operation, where the
timeline of the interchange of messages between the evolver and mixer
threads and evolver and mixer channels is clarified.

The state of the algorithm will be transmitted via messages that
contain data about one population. Since using the whole population
will incur in a lot of overhead, we use a strategy that is inspired in {\em EDA}, 
or Estimation of Distribution Algorithm: instead of 
transmitting the entire population, the message sent to the channel 
will consist of a prototype array containing the probability distribution 
across each gene in the population. In this sense,
this strategy is similar to the one presented by de la Ossa et
al. in \cite{10.1007/978-3-540-30217-9_25}. 

Nonetheless, our strategy differs from a pure EDA in that once the evolutionary
thread have internally run a canonical genetic algorithm, it takes 
only the top quartile of best individuals to compute an array with the 
probability distribution of their genes (computed with frequentist rules) 
and then compose the message that is sent to the {\em mixer} threads. 

A {\em mixer} thread, in turn, 
builds a new prototype array by choosing randomly at each gene location
one probability parameter out of the  two {\em populations} (actually, distributions), 
instead of working directly on individuals. While in the baseline strategy the
selection took place in the mixer thread by eliminating half the
population, in this new design the selection occurs in the evolutionary thread
that selects the 25\% best individuals to compose the probability distribution
message. When the evolver thread reads the message back, it generates a new population 
using the mixed distribution obtained by the mixer.

The introduction of the parallel execution monitor in Comma by Edument, the company that sells it, along with a module, Log::Timeline, that needs to be added to the program in order for it to be able to visualize the results, gives us the capability of seeing events as they are happening in an unified timeline, and also check the duration and simultaneity of the tasks occurring in different threads. Please refer to Figure \ref{fig:comma}, which is a capture of this tool running on the program presented in this paper.

\begin{figure*}[h!tb]
  \centering
  \includegraphics[width=1.9\columnwidth]{imgs/comma}
  \caption{Comma monitor for parallel execution. The $x$ axis is the timeline in seconds, while several concurrent tasks and events are represented in the $y$ axis. The two stripes in different shades of green are the two threads that are running an evolutionary algorithm; the change of color indicates that the task has finished and started a new with new data from the communication channel that conveys populations.}
    \label{fig:comma}
    \end{figure*}

    This initial visualization already highlighted some problems: there were gaps where no thread was being executed, they run almost in parallel, and there were also some {\em short} runs for which we had no explanation. We'll interpret this and show how it was fixed right next.

    \section{Implementation matters: improving the implementation of a concurrent evolutionary algorithm}

By using the log tool to register (and visualize) events in this case, we realized that the {\tt while} loop we were using for the evolutionary algorithm within every task sometimes finished before the condition was met, that is, before either the number of generations (16) had been reached or the solution had been found.

This revealed one of the problems of concurrent implementations: using some shared information, in this case the number of generations that the evolutionary algorithm has run, can be tricky, and the optimizations that the language runs internally can somethimes throw loops off whack, as was the case. Besides, the register revealed an additional problem: not only the loops running the EA finished sometimes before they should, they generally finished much later than they should have. That meant that the EA was running many more generations, getting stuck in local optima, which was making the implementation {\em worse} than the algorithm should have been. This, again, proves that implementation matters and that best practices must be followed to find the best implementation match for the algorithm features.

In this case, the improvement consisted in changing the {\tt while} loop to a {\tt for} loop. The loop variable was running over the required number of generations, and exceptionally it was getting out of the loop when the solution was found. After doing this, we used again the Comma monitor on the program, with a result similar to the one shown in Figure \ref{fig:comma2}. 
%
\begin{figure}[h!tb]
  \centering
  \includegraphics[width=0.95\columnwidth]{imgs/comma}
  \caption{Comma monitor for parallel execution after changing to a {\tt for} loop. It shows in the vertical axis the two threads running, as well as the third one, the mixer thread. The arrows represent events, with {\em Weird} indicating when the loop has exited before it should.}
    \label{fig:comma2}
  \end{figure}
  % 
  One of the things that can be observed is that the whole run takes less, before it never finished before the 16 seconds that were shown in the default window, now it always takes shorter for the EA to find the solution to the Leading Ones problem. Before, the two threads were initiated at the same time and ended the 16 generation at the same time, which did not really make sense, since there's a small overhead involved in starting them. Now it's clear that the second thread starts a bit after the first, and that they drift away after that. The solution is found by one of them after 9 seconds, and communication to channels, shown by arrows, takes place asynchronously, as well as the (very short) execution of the mixer (barely seen in the lowest stripe). There are no gaps in the evolutionary stripes, so those threads are used to its full extent, all of the time.

  This improvement in the execution drove us to try, now, measuring the scaling capabilities once the number of threads are increased. We'll see the results next.

  \section{Results}
    
\section{Conclusions and discussion}
\label{sec:conclusions}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgements}

Hidden for double-blind review
% This paper has been supported in part by projects DeepBio (TIN2017-85727-C4-2-P).

\bibliographystyle{ACM-Reference-Format}
\bibliography{geneura,concurrent}

\end{document}
