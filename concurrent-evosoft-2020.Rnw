%\documentclass[sigconf, authordraft]{acmart} -*- mode: Latex -*-
\documentclass[sigconf]{acmart}
\usepackage[utf8]{inputenc}
\usepackage{booktabs} % For formal tables
\usepackage{tikz}
\usepackage{pgfbaselayers}
\usepackage[underline=false]{pgf-umlsd} % for sequencediagram



% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
\acmDOI{10.1145/nnnnnnn.nnnnnnn}

% ISBN
\acmISBN{978-x-xxxx-xxxx-x/YY/MM}

% Conference
\acmConference[GECCO '20]{the Genetic and Evolutionary Computation Conference 2020}{July 8--12, 2020}{Cancun, Mexico}
\acmYear{2020}
\copyrightyear{2020}

%\acmArticle{4}
\acmPrice{15.00}
% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}

\begin{document}

<<setup, cache=FALSE,echo=FALSE>>=
suppressPackageStartupMessages({
    library(ggplot2)
    library(ggthemes)
})
@

\title{Implementation matters, also in concurrent evolutionary algorithms}

%%% The submitted version for review should be ANONYMOUS
\author{Ben Trovato}
\authornote{Dr.~Trovato insisted his name be first.}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \postcode{43017-6221}
}
\email{trovato@corporation.com}

\author{G.K.M. Tobin}
\authornote{The secretary disavows any knowledge of this author's actions.}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \streetaddress{P.O. Box 1212}
  \city{Dublin}
  \state{Ohio}
  \postcode{43017-6221}
}
\email{webmaster@marysville-ohio.com}

\author{Lars Th{\o}rv{\"a}ld}
\authornote{This author is the
  one who did all the really hard work.}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \streetaddress{1 Th{\o}rv{\"a}ld Circle}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

% The default list of authors is too long for headers.
\renewcommand{\shortauthors}{B. Trovato et al.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
  Concurrent evolutionary algorithms give researchers a chance to leverage the performance of powerful multi-core desktop architectures by parallelizing tasks using a high-level interface. However, this introduces additional complexity at the model and the implementation level. In this paper we describe how to use parallel execution monitoring tools to check the effective parallelism of the implementation, and show how implementation-level improvements can translate in improvements at the algorithmic level, from the very basic evaluations/seconds point of view, to the possible scaling that can be achieved, up to a superlinear scaling in an off-the-shelf platform. We will show how the implementation using communicating sequential processes implemented in the language Raku is the basis for these improvements.
\end{abstract}

\begin{CCSXML}
  <ccs2012>
<concept>
<concept_id>10003752.10003809.10003716.10011136.10011797.10011799</concept_id>
<concept_desc>Theory of computation~Evolutionary algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>

<concept>
<concept_id>10010520.10010521.10010537.10003100</concept_id>
<concept_desc>Computer systems organization~Cloud computing</concept_desc>
<concept_significance>500</concept_significance>
</concept>

<concept>
<concept_id>10010147.10010919.10010172</concept_id>
<concept_desc>Computing methodologies~Distributed algorithms</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}


\ccsdesc[500]{Theory of computation~Evolutionary algorithms}

\ccsdesc[300]{Computing methodologies~Distributed algorithms}

\keywords{Concurrent algorithms, distributed computing,
  event-based systems, stateless algorithms, CSP, Communicating Sequential Processes, 
  algorithm implementation, performance evaluation, distributed
  computing,  heterogeneous distributed systems,
  serverless computing, functions as a service.}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Concurrent computing gives us the capability to think about work done as a series of tasks whose life cycle overlap and are logically independent from each other. Concurrent languages and systems, besides, enables us to actually run those tasks in parallel, given the right combination of operating system and hardware. While not so long ago concurrent computing was the province of clusters or GPUs, current computing and operating systems, paired with concurrent languages such as Raku, Erlang, Scala, Rust or Go, provide any practitioner with entry-level concurrent computing capabilities. As Rob Pike states in \cite{pike2012concurrency},
\begin{quote}
  Concurrency is about structure, parallelism is about execution.
\end{quote}

While concurrency deals with tasks as high-level structures, and provides for ways in which these tasks can synchronize, it may or may not be executed at the same time using the facilities that the OS/system architecture provide.

However, despite concurrency-capable architectures being the prevalent kind of computer systems nowadays, concurrent evolutionary algorithms have not enjoyed the same kind of popularity. A bibliography search of concurrent evolutionary (or genetic) algorithms returns just a few hits, and even if we use another, lower-level term for it, multi-threaded evolutionary algorithms, we will just find a few more, and in this case focused on the use of the multiple theads in a GPU \cite{kromer2011comparison}.

There might be several reasons for that. The main one is that conceiving a concurrent algorithm is not straightforward. A strategy for dividing work among different threads (every one in a task) and communication between them must be established; for starters tasks can be homogeneous or heterogeneous, or some space in between (with some tasks identical and some others doing other, different kind of work, for instance); they can also be synchronous (all tasks doing roughly the same amount of work, or exactly the same, with checkpoints where the main process waits for all tasks to be completed), or asynchronous with tasks starting, ending and communicating out of any pre-established sync mechanism.

A secondary reason, in the case of evolutionary algorithms, is that, being as they are population-based algorithms that must keep a strict exploration-exploitation balance anything afecting the population will tip the balance in one direction or another. Dividing the population will make the algorithm more exploitative, increasing the population more explorative; the more straighforward strategy of dividing population among threads will likely tip the balance in one direction or the other.

A third and probably more important reason, however, is the lack of tools to prove that the algorithm and its implementation is providing the kind of parallelism we really want, by keeping all cores busy running code, with no big gaps. This makes measurements or decisions such as the right number of cores purely heuristic, with nothing more than the result to support it.

So far, this was the path we had been following \cite{merelo:WEA:anon,Merelo:2018:MEA:3205651.3208317:anon}, with diverse results. Those papers proposed a concurrent evolutionary algorithm in Raku that distributed short-burst evolutionary processes in threads, with an additional mixer thread used for combinating populations emanatining from different threads, and a communication system that sent a statistical representation of the population instead of the whole population. In particular, when scaling was apparently not happening, it was difficult to know if it was due to gaps in the use of processors, or to inherent problems with the algorithm itself.

In this paper, after the release of a process logging and monitoring tool for Raku concurrent programs within the Comma integrated development environment, we will examine the correct working of the concurrent evolutionary algorithm; this had not been possible so far, due to the lack of appropriate tools. This will lead to a refactoring of the implementation, if needed, and a re-examination of the scaling capabilities of the system.

The rest of the paper is organized as follows: next we examine the state of the art in the area of parallel and concurrent evolutionary algorithms, followed by a description of the methodology we have followed in the experiments and the experimental setup we have created for this in \ref{sec:design}. We will them present the results in the next section, to be followed by the conclusions.

\section{State of the art}
\label{sec:soa}

Concurrency gives practitioners a practical way to tackle parallelism, and parallelism has been used in the evolutionary algorithms since early on. One of the first papers was published by Cohoon et al. \cite{cohoon1987punctuated}, and it already acknowledged the problems of the exploitation/exploration balance by proposing an algorithm based on {\em punctuated equilibrium}, that is, a balance that is regularly kept, but tipped from time to time. In the distributed architecture they propose, they also consider that parallel evolutionary algorithms should not only aim at being faster by doing the same amount of things in parallel, but try to ``find better solutions with less total work'', that is, the distributed architecture itself will have an algorithmic role and influence positively the performance of the evolutionary algorithm. Ultimately, it's a synchronous island model with random migration between islands, but the underlying ideas seem surprisingly modern and relevant. 


The lack of suitably systems impeded advancement on this subject, but the 90s saw the introduction of the Inmos Transputer, a processor architecture designed for parallelism and that was paired with a programming language called Occam that took advantage of that concurrency. It was used for evolutionary algorithms by East and McFarlane, \cite{east1993implementation}. They emphasize the importance of implementation, by stating that (inherent) parallelism in the algorithm must be considerated separately from its implementation. In the case of population-based algorithms like EA, the obvious approach seems to be to divide the population, or somehow evaluate it in parallel. That is not necessarily the most efficient way, and they mention the possibility of modifying the algorithm to ``render (the) implementation more efficient''. Interestingly, they introduce the so-called communicating process architecture, an implementation of Hoare's Communicating Sequential Processes model \cite{Hoare:1978:CSP:359576.359585} where tasks communicate only through {\em channels}. They do examine the efficiency of every one of the processing elements, realizing that when 8 processors are used, it goes down a bit to 95\%. But their striking result is that the right parallel implementation provides a boost to the algorithm itself. Implementation matters \cite{DBLP:conf/iwann/MereloRACML11}, and by leveraging its capabilities and using the right paradigm (as opposed to a direct reinterpretation of the original sequential algorithm, running in parallel), algorithmic gains can be readily obtained.

It was not until the introduction of concurrent languages on commodity hardware that this paradigm really started to catch on, with EAs using channels implemented in Erlang \cite{AmandaBienz2011,kerdprasop2012concurrent,barwell2017using,DBLP:conf/gecco/CruzGGC13}, Go \cite{PEDROSO2017995} or Raku (Perl 6) \cite{DBLP:conf/gecco/GuervosV18}. The different implementations reflect the facilities provided by the different languages, but also show how they try to measure the effectivity of the concurrency and parallelism on the algorithm. Bienz et al. \cite{AmandaBienz2011} first propose different algorithms, one of with works on a ``grid'', with processes only communicating with others in neighboring nodes of the grid; but they also test on different types of hardware, a 4 and 16 core systems. They show, for instance, that including a {\em master} task that distributes work blurs the distinction between the number of processors, hindering scaling; but due to lack of tools, they can't really find out if it is due to tasks sitting idle waiting for others; in the same way, Kerdprasop et al. \cite{kerdprasop2012concurrent} test the limits of concurrency by showing that performance might suffer when the number of tasks goes over a certain limit (which might be related to the hardware used). Incidentally, the scaling shown by their application is rather poor, with the numbers of threads not decreasing significantly the wallclock time taken by the algorithm; this again shows the need to delve into the actual use by the implementation of the parallel facilities through the concurrent structures provided by the language. The solution proposed by Pedroso et al., \cite{PEDROSO2017995}, uses parallel tasks that communicate via channels, which is a default feature in Go, but there are checkpoints where all tasks are awaited and the code runs sequentially. Even so, they report a good, even superlinear, speedup for up to roughly 15 processors. This is a characteristic of many parallel implementatations, something that was observed quite early on \cite{Alba02parallelevolutionary} and which is due to carefully keeping diversity high. In this case, however, there's no examination of the implementation beyond heuristic measurements.

In this paper we will try to use that kind of tools to find out the source of the bottleneck in previous implementations of the concurrent evolutionary algorithm, and, from that foundation, improve it to achieve superlinear scaling given the right hardware platform.


\section{Design of a concurrent evolutionary algorithm in Raku}
\label{sec:design}

Raku is a concurrent, functional language
\cite{merelo2019running} (formerly known as Perl 6) which was conceived with the
intention of providing a solid conceptual framework for multi-paradigm
computing, including thread-based concurrency and asynchrony. It's got
a heuristic layer that optimizes code during execution time. In the last few
years, performance of programs written in Raku has been sped-up by a 100x factor, 
approaching the same scale of other interpreted
languages, although still with some room for improvement.

The {\tt Algorithm::Evolutionary::Simple} Raku module was published
in the ecosystem a year ago and got recently into version 0.0.7. It
is a straightforward implementation of a canonical evolutionary
algorithm with binary representation and includes building blocks for
a generational genetic algorithm, as well as some fitness functions
used generally as benchmarks.

% Those evolutionary algorithm building blocks do not include concurrent
% primitives; it's up to the developer to design a concurrent
% evolutionary algorithm using it.

The baseline we are building upon, is similar to the one used in previous experiments
\cite{Merelo:2018:MEA:3205651.3208317:anon}. Our intention was to
create a system that was not functionally equivalent to a sequential
evolutionary algorithms, that also follows the principle of
CSP. We decided to allow the algorithm to implement several threads communicating state through
channels. Every process itself will be stateless, reacting to the
% Please consider for other papers: Every 'subroutine' itself will be stateless, reacting to the... - Mario
presence of messages in the channels it is listening to and sending
result back to them, without changing state.

As in the previous papers \cite{merelo:WEA:anon}, 
we will use two groups of threads and two channels. 
The two groups of threads perform the following functions:\begin{itemize}
\item The {\em evolutionary} threads will be the ones performing 
the operations of the evolutionary algorithm.
\item The {\em mixing} thread will take existing populations, to create
  new ones as a mixture of them.
\end{itemize}

\begin{figure}[h!tb]
  %\centering
  \vspace{-.5\intextsep}
\hspace*{-.8\columnsep}
\includegraphics[width=0.95\columnwidth]{imgs/popmixer}
\caption{General scheme of operation of channels and thread groups. }
\label{fig:scheme}
\end{figure}


Besides, the two channels carry messages consisting of populations,
but they do so in a different way:\begin{itemize}
  
\item The {\em evolutionary} channel will be used for carrying
  non-evolved, or generated, populations.
\item The {\em mixer} channel will carry, {\em in pairs}, evolved
  populations. 
\end{itemize}

These will be connected as shown in Figure \ref{fig:scheme}. The
evolutionary thread group will read only from the evolutionary channel,
evolve for a number of generations, and send the result to the mixer
channel; the mixer group of threads will read only from the mixer
channel, in pairs. From every pair, a random element is put back into
the mixer channel, and a new population is generated and sent back to
the evolutionary channel. 

\begin{figure}[h!tb]
  \centering
  \vspace{-.5\intextsep}
\hspace*{-.8\columnsep}
\scalebox{.6}{
\begin{sequencediagram}

\newthread[red]{E}{Evolver} 

\tikzstyle{inststyle}+=[rounded corners=3mm] 
\newinst{C}{Channel}

\tikzstyle{inststyle}+=[rounded corners=0]
\newthread[blue]{M}{Mixer}

\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_1$}{C} 
\mess{C}{$pop_1$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_2$}{C}  
\mess{C}{$pop_2$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{}\end{call}

\postlevel\postlevel
\setthreadbias{west}
\begin{messcall}{M}{\shortstack{ \{\ $mixpop_1$,\\ $mixpop_2$,\\ \vdots \\ $mixpop_k$ \} }}{C}

\mess{C}{$mixpop_1$}{E} 
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_3$}{C} 
\postlevel
\mess{C}{$mixpop_2$}{E} 
%\prelevel
\mess{C}{$pop_3$}{M}
\end{messcall}

\prelevel\prelevel
\begin{call}{E}{evolve()}{E}{}\end{call}

\setthreadbias{east}
\begin{messcall}{E}{$pop_4$}{C}
\mess{C}{$pop_4$}{M}
\end{messcall}

\setthreadbias{west}
\prelevel
\mess{C}{$mixpop_k$}{E}%\end{messcall}

\end{messcall}

\prelevel\prelevel
\begin{call}{M}{mix()}{M}{\vdots}\end{call}
\prelevel
\begin{call}{E}{evolve()}{E}{\vdots}\end{call}

\end{sequencediagram}
}

\caption{Communication between threads and channels for
  concurrent EAs. The two central bars represent the channel, and
  color corresponds to their roles: blue for mixer, red for evolver. 
  Notice how the evolver threads always read from the mixer channel, 
  and always write to the evolver channel.}
\label{fig:schematic}
% \end{figure}
\end{figure}

The main objective of using two channels is
to avoid deadlocks; the fact that one population is written always
back to the mixer channel avoids starvation of the channel. 
Figure \ref{fig:schematic} illustrates this operation, where the
timeline of the interchange of messages between the evolver and mixer
threads and evolver and mixer channels is clarified.

The state of the algorithm will be transmitted via messages that
contain data about one population. Since using the whole population
will incur in a lot of overhead, we use a strategy that is inspired in {\em EDA}, 
or Estimation of Distribution Algorithm: instead of 
transmitting the entire population, the message sent to the channel 
will consist of a prototype array containing the probability distribution 
across each gene in the population. In this sense,
this strategy is similar to the one presented by de la Ossa et
al. in \cite{10.1007/978-3-540-30217-9_25}. 

Nonetheless, our strategy differs from a pure EDA in that once the evolutionary
thread have internally run a canonical genetic algorithm, it takes 
only the top quartile of best individuals to compute an array with the 
probability distribution of their genes (computed with frequentist rules) 
and then compose the message that is sent to the {\em mixer} threads. 

A {\em mixer} thread, in turn, 
builds a new prototype array by choosing randomly at each gene location
one probability parameter out of the  two {\em populations} (actually, distributions), 
instead of working directly on individuals. While in the baseline strategy the
selection took place in the mixer thread by eliminating half the
population, in this new design the selection occurs in the evolutionary thread
that selects the 25\% best individuals to compose the probability distribution
message. When the evolver thread reads the message back, it generates a new population 
using the mixed distribution obtained by the mixer.

\section{Conclusions and discussion}
\label{sec:conclusions}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Acknowledgements}

Hidden for double-blind review
% This paper has been supported in part by projects DeepBio (TIN2017-85727-C4-2-P).

\bibliographystyle{ACM-Reference-Format}
\bibliography{geneura,concurrent}

\end{document}
